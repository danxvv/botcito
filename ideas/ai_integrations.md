# AI/LLM Integration Ideas for DiscordBotcito

Research document exploring creative AI-powered features beyond the existing GameAgent, voice conversation, and TTS capabilities.

---

## Table of Contents

1. [AI-Powered Music Discovery & Curation](#1-ai-powered-music-discovery--curation)
2. [Mood-Aware DJ System](#2-mood-aware-dj-system)
3. [AI DJ Announcer & Transitions](#3-ai-dj-announcer--transitions)
4. [Conversational Music Control](#4-conversational-music-control)
5. [Per-User Music Taste Profiles](#5-per-user-music-taste-profiles)
6. [AI Lyrics Lookup & Sing-Along](#6-ai-lyrics-lookup--sing-along)
7. [Voice Channel Summarizer](#7-voice-channel-summarizer)
8. [Now Playing Art Generator](#8-now-playing-art-generator)
9. [Smart Autoplay with Feedback Learning](#9-smart-autoplay-with-feedback-learning)
10. [Multi-Agent Music Critic](#10-multi-agent-music-critic)
11. [RAG-Powered Game Wiki Assistant](#11-rag-powered-game-wiki-assistant)
12. [Voice Emotion Detection](#12-voice-emotion-detection)
13. [AI Playlist Narrator / Storyteller](#13-ai-playlist-narrator--storyteller)
14. [Song-to-Song Transition Reasoning](#14-song-to-song-transition-reasoning)
15. [Guild Personality / Bot Character System](#15-guild-personality--bot-character-system)

---

## 1. AI-Powered Music Discovery & Curation

**Description:**
Go beyond YouTube Music's `get_watch_playlist` by using an LLM to generate creative music recommendations based on natural language descriptions. Users could say `/discover chill lo-fi beats for studying at 2am` or `/discover songs that sound like Radiohead meets Bjork` and get back curated results that no traditional recommendation engine would surface.

The LLM would generate search queries, evaluate results from YouTube Music, and rank them by relevance to the user's description -- acting as a semantic music search layer on top of the existing ytmusicapi.

**Why it's cool:**
- Traditional recommendation engines are limited to collaborative filtering ("people who listened to X also listened to Y"). LLMs understand *vibes*, *moods*, and *cross-genre connections* in ways algorithms cannot.
- Enables completely novel query types: "songs my dad would have listened to in the 80s", "battle music for a D&D session", "the saddest song ever written".

**Feasibility:** High -- the bot already has OpenRouter integration and ytmusicapi. This is mostly prompt engineering plus a search-and-rank loop.

**Suggested libraries/APIs:**
- Existing: `agno` Agent with `ytmusicapi` as a tool
- `OpenRouter` (already integrated) for the LLM reasoning layer
- Could add Spotify Web API (via `spotipy`) for richer metadata (audio features like danceability, energy, valence)

**Complexity:** Medium

---

## 2. Mood-Aware DJ System

**Description:**
Analyze the "mood" of the voice channel and current conversation to automatically adjust music selection. The system would combine multiple signals:
- Time of day (late night = chill, afternoon = upbeat)
- Chat sentiment analysis (if people are hyped in text chat, play energetic music)
- Current song's audio features (maintain or shift the energy curve)
- Explicit mood commands (`/mood chill`, `/mood hype`, `/mood melancholy`)

An Agno agent would maintain a "mood state" per guild and use it to influence autoplay selections, creating a coherent listening session rather than random song chains.

**Why it's cool:**
- Makes the bot feel *aware* of the room rather than just being a jukebox
- Creates a "virtual DJ" experience where the music naturally flows with the group's energy
- The bot could announce mood shifts: "The vibe is shifting -- let me find something to match"

**Feasibility:** High -- sentiment analysis and time-of-day logic are straightforward. The harder part is building a good mood-to-music mapping, but LLMs excel at this.

**Suggested libraries/APIs:**
- `agno` Agent for mood reasoning
- `ytmusicapi` search with mood-aware queries
- Spotify Audio Features API (energy, valence, danceability) via `spotipy` for quantitative mood matching
- `TextBlob` or LLM-based sentiment analysis on recent chat messages

**Complexity:** Medium-High

---

## 3. AI DJ Announcer & Transitions

**Description:**
When songs transition (especially during autoplay), have the bot generate and speak short DJ-style announcements using TTS. Examples:
- "That was 'Bohemian Rhapsody' by Queen. Coming up next, a deep cut from the same era..."
- "You've been listening for an hour now -- here's something to keep the energy going!"
- Occasional fun facts about the artist or song pulled via web search

The announcements would be generated by an LLM, rewritten for natural speech (reusing the existing `_rewrite_for_voice` pattern), and spoken via Qwen3-TTS between songs.

**Why it's cool:**
- Transforms a simple music queue into a radio station experience
- Uses existing infrastructure (Qwen3-TTS, OpenRouter, Agno) in a novel way
- Could develop a "DJ personality" that evolves based on the server's music taste

**Feasibility:** High -- all the building blocks exist. The bot already has TTS, voice playback with music ducking, and LLM access.

**Suggested libraries/APIs:**
- Existing: `Qwen3TTSProvider`, `VoiceConversation.speak_text()`, `OpenRouter`
- Exa MCP for quick artist/song trivia lookups
- A new `DJAnnouncerAgent` in the Agno team framework

**Complexity:** Medium

---

## 4. Conversational Music Control

**Description:**
Extend the existing voice conversation (`/talk`) to understand music commands naturally. Instead of requiring slash commands, users could say:
- "Hey bot, play something by Arctic Monkeys"
- "Hey bot, skip this song"
- "Hey bot, turn it up"
- "Hey bot, what's playing right now?"
- "Hey bot, add this to my playlist"

The existing wake-word system (`WAKE_PHRASES`) already handles transcription. This would add an intent classifier that routes between "gaming question" and "music command" before hitting the GameAgent.

**Why it's cool:**
- Hands-free music control while gaming -- the dream use case for a Discord music bot
- Natural language removes the friction of slash commands
- The bot already listens and transcribes; this just adds smarter routing

**Feasibility:** High -- the voice pipeline exists. Adding an intent classifier (even just a prompt to the existing LLM) to distinguish "music control" from "gaming question" is straightforward.

**Suggested libraries/APIs:**
- Existing: `VoiceConversation`, `GameAgent.transcribe_audio()`, `MusicPlayerManager`
- Agno agent or simple LLM prompt for intent classification
- Could use structured output (JSON) from the LLM to extract commands: `{"intent": "play", "query": "Arctic Monkeys"}`

**Complexity:** Medium

---

## 5. Per-User Music Taste Profiles

**Description:**
Build persistent taste profiles for each user based on their listening history, likes/dislikes (already tracked via `/like` and `/dislike`), and explicit preferences. The profile would be stored in SQLite and used to:
- Generate personalized recommendations: `/recommend` gives each user different results
- Blend multiple users' tastes when they're all in a voice channel: "Playing music that everyone here would enjoy"
- Show taste compatibility between users: `/compatibility @user1 @user2`

An LLM would periodically summarize each user's listening patterns into a natural-language taste profile (e.g., "Prefers alternative rock and electronic music, avoids country, listens most to upbeat tracks in the evening").

**Why it's cool:**
- Makes autoplay dramatically smarter for repeat users
- Social features (compatibility scores, shared taste discovery) drive engagement
- The natural-language profile doubles as context for the LLM when generating recommendations

**Feasibility:** High -- the bot already tracks play history and ratings in SQLite. The profile generation is an LLM summarization task.

**Suggested libraries/APIs:**
- Existing: `audit.database` (play history), `ratings` module, SQLite
- `agno` Agent with `enable_agentic_memory=True` for profile persistence
- Embeddings via OpenAI/OpenRouter for taste-vector similarity (cosine similarity between user profiles)

**Complexity:** Medium

---

## 6. AI Lyrics Lookup & Sing-Along

**Description:**
Add a `/lyrics` command that fetches and displays lyrics for the currently playing song. Beyond basic lookup, the AI layer could:
- Translate lyrics to the server's language on the fly
- Explain the meaning of complex lyrics ("what does this verse mean?")
- Generate synchronized lyrics display in Discord (updating the embed as the song plays)
- Karaoke mode: speak/display lyrics slightly ahead of the music for sing-along

**Why it's cool:**
- Lyrics are one of the most requested features in music bots
- AI translation and explanation add unique value over existing lyrics bots
- Karaoke mode in Discord voice channels is genuinely novel

**Feasibility:** Medium -- lyrics fetching via web search (Exa) or APIs (Genius, Musixmatch) is straightforward. Synchronized display is harder due to timing.

**Suggested libraries/APIs:**
- Exa MCP (already integrated) for lyrics web search
- `lyricsgenius` Python package for Genius API
- OpenRouter LLM for translation and meaning explanation
- For sync: would need to parse LRC format or estimate timing

**Complexity:** Low (basic lyrics) / High (synchronized karaoke)

---

## 7. Voice Channel Summarizer

**Description:**
Use the existing recording capability (`/record` + `/stoprecord`) to generate AI summaries of voice channel conversations. After a recording session:
- Transcribe all captured audio per user (using the existing `transcribe_audio` method)
- Use an LLM to generate a summary of the conversation
- Identify action items, decisions made, or funny moments
- Post the summary as a Discord embed

Could also work in real-time during `/talk` mode, building a running summary.

**Why it's cool:**
- "What did I miss?" is a universal Discord problem -- this solves it
- Adds real value for gaming guilds that discuss strategies in voice
- Leverages existing recording infrastructure that's already built

**Feasibility:** High -- recording and transcription already work. The summarization is a standard LLM task.

**Suggested libraries/APIs:**
- Existing: `VoiceListener`, `GameAgent.transcribe_audio()`, recording system
- OpenRouter for summarization
- Could use `agno` Agent with `ReasoningTools` for structured summaries

**Complexity:** Medium

---

## 8. Now Playing Art Generator

**Description:**
Generate unique visual art for the currently playing song using image generation APIs. When a new song starts, create a custom "now playing" image based on:
- Song title and artist (text-to-image prompt)
- Mood/genre of the music
- Time of day and listening context

The generated image would be displayed in the now-playing embed, making each song feel unique rather than just showing a YouTube thumbnail.

**Why it's cool:**
- Turns the music experience into a multi-sensory journey
- Every song gets its own unique artwork, making the bot feel premium
- Great for servers that value aesthetics

**Feasibility:** Medium -- image generation APIs (DALL-E, Stable Diffusion, Midjourney) are readily available but add latency and cost.

**Suggested libraries/APIs:**
- OpenAI DALL-E 3 via API (high quality, ~$0.04/image)
- `stability-ai` SDK for Stable Diffusion (can run locally with GPU)
- Agno has multimodal image support (`agno.media.Image`)
- Could cache generated art in SQLite keyed by video_id to avoid regeneration

**Complexity:** Medium

---

## 9. Smart Autoplay with Feedback Learning

**Description:**
Enhance the existing autoplay system with a learning loop. Currently autoplay uses `ytmusicapi.get_watch_playlist()` which gives YouTube's standard recommendations. A smarter system would:
- Track which autoplay songs users skip vs. let play
- Track explicit feedback (existing `/like` and `/dislike`)
- Use this signal to refine future autoplay selections
- LLM generates search queries that avoid patterns the group dislikes
- Over time, the autoplay becomes uniquely tuned to each server's taste

The key insight: instead of just filtering *out* played songs (current approach), actively learn *what works* and generate better search queries.

**Why it's cool:**
- Autoplay that gets smarter over time is a killer feature
- Combines structured data (skip rates, ratings) with LLM reasoning
- Creates a unique listening experience per server that improves with use

**Feasibility:** High -- the data collection infrastructure (ratings, play history) exists. The learning loop is prompt engineering on top of aggregated signals.

**Suggested libraries/APIs:**
- Existing: `YouTubeMusicHandler`, `ratings` module, `audit.database`
- `agno` Agent with memory for the "taste model"
- SQLite for storing autoplay feedback (skip counts, play-through rates)

**Complexity:** Medium

---

## 10. Multi-Agent Music Critic

**Description:**
Create a team of specialist agents (similar to the GameGuide Team pattern) that provide entertaining music commentary:
- **Genre Expert**: Identifies sub-genres and musical influences
- **History Expert**: Places the song in its historical context, explains the era
- **Vibe Analyst**: Describes the mood, energy, and best use cases for the song
- **Hot Take Generator**: Gives spicy, entertaining opinions to spark conversation

Users could invoke this with `/critique` on the current song, and the team would produce a rich, entertaining analysis.

**Why it's cool:**
- Directly extends the existing Agno team pattern (GameGuide Team) to a new domain
- Sparks conversation and engagement in the server
- Educational and entertaining -- learn about music you're listening to

**Feasibility:** High -- the team architecture exists in `game_agent/`. This is mostly new agent configurations and prompts.

**Suggested libraries/APIs:**
- Existing: `agno` Team framework, Exa MCP for research
- Same architecture as `game_agent/team_factory.py`
- `create_music_team()` factory function mirroring `create_game_team()`

**Complexity:** Medium

---

## 11. RAG-Powered Game Wiki Assistant

**Description:**
Upgrade the GameAgent from pure web search (Exa) to a hybrid RAG (Retrieval Augmented Generation) system. For popular games, pre-index wiki content into a vector store so the agent can answer instantly without web search latency:
- Index game wikis (Fandom, game-specific wikis) into embeddings
- Use vector similarity search for instant retrieval
- Fall back to Exa web search for games/topics not in the index
- Allow server admins to add custom game guides: `/addguide <game> <url>`

This creates a fast, accurate knowledge base that improves over time.

**Why it's cool:**
- Dramatically reduces response latency for common questions (vector search vs. web search)
- Builds a growing knowledge base customized to each server's games
- Combines the accuracy of curated content with the breadth of web search

**Feasibility:** Medium -- requires adding a vector store and embedding pipeline. LangChain or Agno's built-in knowledge features can handle this.

**Suggested libraries/APIs:**
- `agno` Knowledge with `search_knowledge=True` (already supported, see docs)
- `agno.db.sqlite.SqliteDb` (already used) for memory persistence
- LangChain `InMemoryVectorStore` or `chromadb` for local vector storage
- `OpenAIEmbeddings` or `sentence-transformers` for embedding generation
- `WebBaseLoader` from LangChain for indexing wiki pages

**Complexity:** High

---

## 12. Voice Emotion Detection

**Description:**
Analyze the emotional tone of users' voice in the channel to provide context-aware responses. When a user says "Hey bot, I keep dying to this boss" in a frustrated tone, the bot could:
- Detect frustration and respond with extra encouragement
- Adjust its speaking tone/pace to be calming
- Suggest taking a break if frustration is high
- Use emotion as a signal for mood-aware music selection (see idea #2)

Modern audio LLMs (like the ones used via OpenRouter) can already detect emotion from audio input. The bot's existing audio pipeline sends raw WAV data to the model.

**Why it's cool:**
- Makes the bot feel genuinely empathetic and aware
- Novel use of multi-modal AI -- few bots analyze voice emotion
- Integrates naturally with the existing voice conversation system

**Feasibility:** Medium -- depends on the audio model's emotion detection accuracy. GPT-5.2-audio-preview and similar models can detect basic emotions from audio.

**Suggested libraries/APIs:**
- Existing: `GameAgent.ask_audio()` already sends audio to the model
- Add emotion analysis prompt: "Also describe the speaker's emotional tone"
- `speechbrain` for local emotion classification (if offline detection is preferred)
- Agno's `Audio` media type for multi-modal input

**Complexity:** Medium

---

## 13. AI Playlist Narrator / Storyteller

**Description:**
Create a "story mode" for playlists where the bot weaves a narrative between songs. Given a playlist or queue, the AI generates a story where each song becomes a chapter:
- "Our journey begins in the rain-soaked streets of London..." (plays a moody track)
- "But then the sun breaks through..." (transitions to an upbeat song)
- The narrative adapts based on what songs are actually in the queue

Could also work in reverse: `/storytime <genre>` generates a story first, then picks songs to match each chapter.

**Why it's cool:**
- Completely unique feature -- no other music bot does this
- Creates an immersive listening experience
- Perfect for D&D sessions, road trip vibes, or themed listening parties
- Uses TTS (Qwen3) to narrate between songs

**Feasibility:** Medium -- the narrative generation is an LLM task. The challenge is timing narration between songs smoothly.

**Suggested libraries/APIs:**
- Existing: `Qwen3TTSProvider`, `VoiceConversation._play_response()`, `OpenRouter`
- Agno Agent for story generation with song metadata as context
- Music ducking (already implemented) for smooth narration over music transitions

**Complexity:** High

---

## 14. Song-to-Song Transition Reasoning

**Description:**
When autoplay picks the next song, have the bot briefly explain *why* it chose that song. Display the reasoning in the now-playing embed:
- "Playing 'Starlight' by Muse because you've been enjoying arena rock tonight"
- "Switching to 'Midnight City' by M83 -- it has a similar synth-driven energy to the last 3 tracks"
- "Going deeper into shoegaze since nobody skipped the last My Bloody Valentine track"

This makes the autoplay system transparent and educational, helping users discover *why* certain songs connect.

**Why it's cool:**
- Transforms autoplay from a black box into a music education tool
- Users learn about musical connections and genres
- Builds trust in the autoplay system ("the bot actually understands my taste")

**Feasibility:** High -- the reasoning can be generated by the same LLM that selects songs. Just add a "explain your choice" step to the autoplay pipeline.

**Suggested libraries/APIs:**
- Existing: `autoplay.py`, OpenRouter, `agno` Agent
- LLM structured output: `{"next_song": "...", "reason": "..."}`

**Complexity:** Low

---

## 15. Guild Personality / Bot Character System

**Description:**
Allow each Discord server to customize the bot's personality via `/personality`. The personality affects:
- How the bot speaks (TTS tone, word choice)
- DJ announcement style (formal radio host vs. chaotic energy vs. chill friend)
- How the GameAgent responds (serious strategist vs. casual gamer vs. meme lord)
- Language and cultural references

Example personalities:
- **Radio Host**: Professional, smooth transitions, trivia between songs
- **Hype Beast**: Maximum energy, lots of exclamation marks, hypes up every song
- **Lo-fi Chill**: Minimal talking, calm suggestions, ambient vibes
- **Gaming Buddy**: Casual, uses gaming slang, references memes
- **Custom**: Server admins write their own personality prompt

The personality would be stored in the settings database (already has `settings.py`) and injected into all agent system prompts.

**Why it's cool:**
- Every server gets a unique bot experience
- Deeply personalized -- the bot *feels* different in each community
- Low implementation cost (it's essentially a system prompt modifier) with high impact

**Feasibility:** High -- this is primarily prompt engineering plus a settings entry.

**Suggested libraries/APIs:**
- Existing: `settings.py` (SQLite settings), `game_agent/config.py` (agent instructions)
- Store personality as a text blob in settings DB
- Inject into all agent `instructions` arrays at runtime

**Complexity:** Low-Medium

---

## Summary Matrix

| # | Feature | Complexity | Effort | Impact | Dependencies |
|---|---------|-----------|--------|--------|-------------|
| 1 | AI Music Discovery | Medium | ~2 days | High | ytmusicapi, OpenRouter |
| 2 | Mood-Aware DJ | Medium-High | ~4 days | High | Sentiment analysis, Spotify API |
| 3 | DJ Announcer | Medium | ~2 days | Medium | Qwen3-TTS, OpenRouter |
| 4 | Conversational Music Control | Medium | ~3 days | Very High | Voice pipeline, intent classifier |
| 5 | User Taste Profiles | Medium | ~3 days | High | SQLite, embeddings |
| 6 | AI Lyrics | Low-High | ~1-4 days | Medium | Genius API or Exa |
| 7 | Voice Summarizer | Medium | ~2 days | Medium | Transcription, OpenRouter |
| 8 | Now Playing Art | Medium | ~2 days | Medium | DALL-E or Stable Diffusion |
| 9 | Smart Autoplay | Medium | ~3 days | Very High | Ratings data, LLM feedback loop |
| 10 | Music Critic Team | Medium | ~2 days | Medium | Agno teams |
| 11 | RAG Game Wiki | High | ~5 days | High | Vector store, embeddings |
| 12 | Voice Emotion | Medium | ~3 days | Medium | Audio models |
| 13 | Playlist Storyteller | High | ~4 days | High | TTS, LLM, timing |
| 14 | Transition Reasoning | Low | ~1 day | Medium | Autoplay pipeline, LLM |
| 15 | Guild Personality | Low-Medium | ~1 day | High | Settings DB, prompt engineering |

## Recommended Starting Points

These three features offer the best ratio of impact to implementation effort, and they build on infrastructure that already exists in the codebase:

1. **Guild Personality System (#15)** -- Lowest effort, highest "wow factor". Just a settings entry and prompt injection. Start here.
2. **Conversational Music Control (#4)** -- The voice pipeline already exists. Adding intent classification to route between music commands and gaming questions transforms the bot.
3. **Smart Autoplay with Feedback Learning (#9)** -- The ratings and play history data already exists in SQLite. Adding an LLM layer to reason about this data makes autoplay genuinely intelligent.

After those three, **AI Music Discovery (#1)** and **DJ Announcer (#3)** are natural next steps that compose well with the above features.
